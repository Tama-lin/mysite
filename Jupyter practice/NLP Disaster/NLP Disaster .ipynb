{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./nlp-getting-started/train.csv\")\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(sentence):\n",
    "    sentence = str(sentence).lower() # lowercase\n",
    "    sentence = re.sub(r'\\d+', '', sentence) # Clean numbers\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\") # Clean white spaces\n",
    "    sentence = tokenizer.tokenize(sentence) \n",
    "    sentence = np.atleast_1d(np.squeeze(np.array([tweet_tokenizer.tokenize(item) for item in sentence]))) # Tokenization\n",
    "    stemmer= PorterStemmer() # stem\n",
    "#     lemmatizer=WordNetLemmatizer()\n",
    "    sentence=np.atleast_1d(np.array([stemmer.stem(item) for item in sentence]).squeeze())\n",
    "#     sentence=np.atleast_1d(np.array([lemmatizer.lemmatize(item) for item in sentence]).squeeze())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=[]\n",
    "for sentence in data[\"keyword\"].values:\n",
    "    tokenized.append(clean_string(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nan', 'ablaz', 'accid', 'aftershock', 'airplan', 'ambul',\n",
       "       'annihil', 'apocalyps', 'armageddon', 'armi', 'arson', 'arsonist',\n",
       "       'attack', 'avalanch', 'battl', 'bioterror', 'blaze', 'bleed',\n",
       "       'blew', 'blight', 'blizzard', 'blood', 'bloodi', 'blown', 'bodi',\n",
       "       'bomb', 'bridg', 'build', 'burn', 'bush', 'casualti', 'catastroph',\n",
       "       'chemic', 'cliff', 'collaps', 'collid', 'collis', 'crash', 'crush',\n",
       "       'curfew', 'cyclon', 'damag', 'danger', 'dead', 'death', 'debri',\n",
       "       'delug', 'demolish', 'demolit', 'derail', 'desol', 'destroy',\n",
       "       'destruct', 'deton', 'devast', 'disast', 'displac', 'drought',\n",
       "       'drown', 'dust', 'earthquak', 'electrocut', 'emerg', 'engulf',\n",
       "       'epicentr', 'evacu', 'explod', 'explos', 'eyewit', 'famin',\n",
       "       'fatal', 'fear', 'fire', 'first', 'flame', 'flatten', 'flood',\n",
       "       'forest', 'hail', 'hailstorm', 'harm', 'hazard', 'heat', 'hellfir',\n",
       "       'hijack', 'hostag', 'hurrican', 'injur', 'injuri', 'inund',\n",
       "       'landslid', 'lava', 'lightn', 'loud', 'mass', 'massacr', 'mayhem',\n",
       "       'meltdown', 'militari', 'mudslid', 'natur', 'nuclear', 'obliter',\n",
       "       'oil', 'outbreak', 'pandemonium', 'panic', 'panick', 'polic',\n",
       "       'quarantin', 'radiat', 'rainstorm', 'raze', 'refuge', 'rescu',\n",
       "       'rescuer', 'riot', 'rubbl', 'ruin', 'sandstorm', 'scream',\n",
       "       'seismic', 'sinkhol', 'sink', 'siren', 'smoke', 'snowstorm',\n",
       "       'storm', 'stretcher', 'structur', 'suicid', 'sunk', 'surviv',\n",
       "       'survivor', 'terror', 'terrorist', 'threat', 'thunder',\n",
       "       'thunderstorm', 'tornado', 'tragedi', 'trap', 'trauma',\n",
       "       'traumatis', 'troubl', 'tsunami', 'twister', 'typhoon', 'upheav',\n",
       "       'violent', 'volcano', 'war', 'weapon', 'whirlwind', 'wild',\n",
       "       'wildfir', 'windstorm', 'wound', 'wreck', 'wreckag'], dtype=object)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokenized)[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
